{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If $Y_1, Y_2, . . . , Y_n$ are random observations from a uniform pdf over $[0, θ]$, both $\\hat{θ_1} =  \\frac{n+1}{n}  · Y{\\max}$ and $\\hat{θ_2} = (n + 1)Y{\\min}$ are unbiased estimators for $θ$. \n",
    "  + Show that $Var(\\hatθ_2)/Var(\\hatθ_1) = n^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $Y_1,...,Y_n \\sim^{i.i.d.} U(0,\\theta)$,$$f_Y(y;\\theta) = \\frac{1}{\\theta}, 0\\leq y \\leq \\theta $$\n",
    "\n",
    "Derive the pdf of $Y_{\\max}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    F_{Y_{\\max}}(y;\\theta) &= P(Y_{\\max}\\leq y) \\\\\n",
    "    &= \\prod_{i=1}^n P(Y\\leq y) \\\\\n",
    "    &= \\prod_{i=1}^n \\frac{y}{\\theta}   \\\\\n",
    "    &= (\\frac{y}{\\theta})^n\n",
    "\\end{aligned}\n",
    "\n",
    "$$f_{Y_{\\max}}(y;\\theta)  = \\frac{n}{\\theta}(\\frac{y}{\\theta})^{n-1} $$\n",
    "\n",
    "Compute $Var(\\hat{\\theta_1})$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    Var(\\hat{\\theta_1}) &= E[(\\frac{n+1}{n}Y_{\\max})^2] - \\theta^2  \\\\\n",
    "    &= (\\frac{n+1}{n})^2 \\int_{0}^{\\theta} \\frac{n}{\\theta^n}y^{n+1}dy - \\theta^2   \\\\\n",
    "    &= (\\frac{n+1}{n})^2 [\\frac{n}{n+2}\\theta^{-n} y^{n+2}]^{\\theta}_0 - \\theta^2  \\\\\n",
    "    &= [\\frac{(n+1)^2}{n(n+2)}-1]\\theta^2 \\\\\n",
    "    &= \\frac{1}{n(n+2)}\\theta^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Correction:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the pdf of $Y_{\\min}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    F_{Y_{\\min}}(y;\\theta) &= P(Y_{\\min}\\leq y)   \\\\\n",
    "    &= 1-\\prod_{i=1}^n P(Y>y)   \\\\\n",
    "    &= 1- \\prod_{i=1}^n (1-\\frac{y}{\\theta})  \\\\\n",
    "    &= 1- (1-\\frac{y}{\\theta})^n\n",
    "\\end{aligned}\n",
    "\n",
    "$$ f_{Y_{\\min}}(y;\\theta) = \\frac{n}{\\theta}(1-\\frac{y}{\\theta})^{n-1} = \\frac{n}{\\theta^{n}}(\\theta - y)^{n-1} $$\n",
    "\n",
    "Compute $Var(\\hat{\\theta_2})$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    Var(\\hat{\\theta_2}) &= E[(n+1)Y_{\\min}]^2   - \\theta^2  \\\\\n",
    "    &= (n+1)^2\\theta^{-n} \\int_{0}^{\\theta} n y^2(\\theta - y)^{n-1}dy -\\theta^2   \\\\\n",
    "    &= (n+1)^2 \\theta^{-n}\\int_{0}^{\\theta} y^2 d({-(\\theta-y)^n}) - \\theta^2   \\\\\n",
    "    &= (n+1)^2 \\theta^{-n}\\{  [y^2 {-(\\theta-y)^n}]^{\\theta}_0 + \\int_{0}^{\\theta}2y {(\\theta-y)^n} dy  \\} -\\theta^2    \\\\\n",
    "    &= (n+1)^2 \\theta^{-n} \\{ 0- [2y \\frac{(\\theta-y)^{n+1}}{(n+1)}]^{\\theta}_0 + \\int_{0}^{\\theta}2\\frac{(\\theta-y)^{n+1}}{(n+1)}dy \\} - \\theta^2  \\\\\n",
    "    &= (n+1)^2 \\theta^{-n}   \\{0-[2\\frac{(\\theta-y)^{n+2}}{(n+1)(n+2)}]^{\\theta}_{0}\\}   - \\theta^2 \\\\\n",
    "    &= (\\frac{2(n+1)}{n+2} - 1) \\theta^2  \\\\\n",
    "    &= \\frac{n}{n+2}\\theta^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have $$  \\frac{Var(\\hat{\\theta_2})}{\\hat{\\theta_1}} = n^2  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theorem 5.5.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (Prove the Cramér-Rao Inequality.) \n",
    "+ Let $f_Y (y;θ)$ be a continuous pdf with continuous first-order and second-order derivatives. Also, suppose that the set of $y$ values, where $f_Y (y;θ)=0$, does not depend on $θ$.\n",
    "+ Let $Y_1, Y_2, . . . , Y_n$ be a random sample from $f_Y (y;θ)$, and let $\\hatθ = h(Y_1, Y_2, . . . , Y_n)$ be any unbiased estimator of θ. Then $$  Var(\\hat{\\theta}) \\geq \\frac{1}{n}(E[\\frac{\\partial \\ln(f_{Y^n}(Y,\\theta))}{\\partial \\theta}]^2)^{-1} = -\\frac{1}{n}(E[\\frac{\\partial^2 \\ln(f_{Y^n}(Y,\\theta))}{\\partial\\theta^2 }])^{-1} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "          \\text{By the Cauchy-Schwarz inequality:} \n",
    "          Cov^2[\\hat{\\theta},\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}] &\\leq Var(\\hat{\\theta})\\times Var[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]  \\\\\n",
    "          Var(\\hat{\\theta}) &\\geq \\frac{Cov^2[\\hat{\\theta},\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]}{Var[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]}   \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Compute $Cov^2[\\hat{\\theta},\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]$:\n",
    "\n",
    "\\begin{aligned}\n",
    "          &Cov^2[\\hat{\\theta},\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}] \\\\\n",
    "          &= E\\{ \\hat{\\theta}\\times \\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta} \\} -E(\\hat{\\theta}) E[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]  \\\\\n",
    "          &= \\int \\hat{\\theta} \\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta} f_{Y^n}(y,\\theta) dy  \\\\\n",
    "          &= \\int \\hat{\\theta} \\frac{\\partial f_{Y^n}(y,\\theta)}{\\partial \\theta} dy  \\\\\n",
    "          \\text{Exchange the order of differentiation and integration:}&= \\frac{\\partial [\\int \\frac{\\partial f_{Y^n}(y,\\theta)}{\\partial \\theta} dy]}{\\partial \\theta} \\\\\n",
    "          &= \\frac{\\partial E(\\hat{\\theta})}{\\partial \\theta} = \\frac{d E(\\hat{\\theta})}{d \\theta}\n",
    "\\end{aligned}\n",
    "\n",
    "Compute $Var[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]$:\n",
    "\n",
    "\\begin{aligned}\n",
    "        &Var[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]  \\\\\n",
    "        &= E[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]^2 - E^2[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]  \\\\\n",
    "        &= E[\\frac{\\partial \\ln f_{Y^n}(y,\\theta)}{\\partial \\theta}]^2 +0 \\\\\n",
    "        &= E[\\frac{\\partial \\ln \\prod_{i=1}^n f_{Y_i}(y,\\theta)}{\\partial \\theta}]^2  \\\\\n",
    "        &= E[\\frac{\\partial \\sum_{i=1}^n\\ln  f_{Y_i}(y,\\theta)}{\\partial \\theta}]^2 \\\\\n",
    "        &= \\sum_{i=j=1}^n E[\\frac{\\partial  \\ln  f_{Y_i}(y,\\theta)}{\\partial \\theta}]^2  + \\sum_{i\\neq j}^n E[\\frac{\\partial  \\ln  f_{Y_i}(y,\\theta)}{\\partial \\theta}\\times \\frac{\\partial f_{Y_j}(y,\\theta)}{\\partial \\theta}]\\\\\n",
    "        &= nE[\\frac{\\partial \\ln f_{Y}(y,\\theta)}{\\partial \\theta}]^2 + \\sum_{i\\neq j}^n E[\\frac{\\partial  \\ln  f_{Y_i}(y,\\theta)}{\\partial \\theta}] E[\\frac{\\partial f_{Y_j}(y,\\theta)}{\\partial \\theta}]  \\\\\n",
    "        &= nE[\\frac{\\partial \\ln f_{Y}(y,\\theta)}{\\partial \\theta}]^2 + 0\\\\\n",
    "        &= n\\times I(\\theta)\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we obtain the Cramer-Rao lower bound:$$  Var(\\hat{\\theta}) \\geq \\frac{d[E(\\hat{\\theta})]}{d\\theta}[n\\times I(\\theta)]^{-1}  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5.1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let $Y_1,Y_2,...,Y_n$ be a random sample from $$f_Y(y;θ) = \\frac1θ e^{−y/θ}, y > 0$$\n",
    "+ Compare the Cramér-Rao lower bound for $f_Y(y;θ)$ to the variance of the maximum likelihood estimator for θ,$$\\hatθ = \\frac1n \\sum^n_{i=1} Y_i$$\n",
    "+ Is $\\bar{Y}$ a best estimator for θ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Fisher information $I(\\theta)$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    I(\\theta) &= -E[\\frac{\\partial^2 \\ln(f(y,\\theta))}{\\partial \\theta^2}]  \\\\\n",
    "    &= -E[\\frac{\\partial^2 (-\\ln\\theta - \\frac{y}{\\theta})}{\\partial \\theta^2}] \\\\\n",
    "    &= -E[\\frac{\\partial (-\\frac{1}{\\theta}+ \\frac{y}{\\theta^2})}{\\partial \\theta}] \\\\\n",
    "    &= -E[\\frac{1}{\\theta^2} -2 \\frac{y}{\\theta^3}]    \\\\\n",
    "    &= \\frac{2}{\\theta^2}-\\frac{1}{\\theta^2}  \\\\\n",
    "    &= \\frac{1}{\\theta^2}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we obtain the Cramer-Rao lower bound:$$Var(\\hat{\\theta})\\geq \\frac{d[E(\\hat{\\theta})]}{d\\theta}[n\\times I(\\theta)]^{-1} = \\frac{\\theta^2}{n} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the CLT, when $n$ is suffientlty large, we have $$\\bar{Y}\\sim^{d} N(\\mu,\\frac{\\sigma^2}{n}) = N(\\theta,\\frac{\\theta^2}{n})$$\n",
    "\n",
    "Thus, $$  Var(\\bar{Y}) = \\frac{\\theta^2}{n} \\leq Var(\\hat{\\theta})  $$\n",
    "\n",
    "$\\bar{Y}$ is a best estimator for the $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 5.5.2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let $X_1, X_2,..., X_n$ be a random sample of size n from the Poisson distribution, $$p_X(k;λ) = \\frac{e^{−λ}λ^k}{k!} ,k = 0,1,.... $$\n",
    "+ Show that $\\hatλ = \\frac1n \\sum^{n}_{i=1} X_i$ is an efficient estimator for λ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log-PMF of $X$:$$l(\\lambda) =  k\\ln(\\lambda) - {\\lambda}$$\n",
    "\n",
    "Compute the score function of $\\lambda$: $$\\frac{\\partial l(\\lambda)}{\\partial \\lambda} = \\frac{k}{\\lambda} - 1 $$\n",
    "\n",
    "Derive the Fisher information of $\\lambda$: $$-E[\\frac{\\partial^2 l(\\lambda)}{\\partial \\lambda^2}] = E(\\frac{k}{\\lambda^2}) = \\frac{1}{\\lambda}  $$\n",
    "\n",
    "Now, we obtain the Cramer-Rao lower bound of $\\lambda$: $$ Var(\\hat{\\lambda}) \\geq \\frac{d[E(\\hat{\\lambda})]}{d\\lambda}[n\\times I(\\theta)]^{-1} = \\frac{\\lambda}{n} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the CLT, when $n$ is sufficiently large, $$ \\hat{\\lambda} = \\bar{X} \\sim^{d} N(\\mu,\\frac{\\sigma^2}{n}) = N(\\lambda,\\frac{\\lambda}{n}) $$\n",
    "\n",
    "Thus, $  Var{\\hat{\\lambda}} = B(\\lambda)    $, $\\hat{\\lambda} = \\bar{X}$ is an efficient estimator for $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 5.5.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let $Y_1,Y_2,...,Y_n$ be a random sample of size n from the pdf $$ f_Y(y;θ) =\\frac{1}{(r − 1)!θ^r}y^{r−1}e^{−y/θ}, y > 0$$\n",
    "  1. Show that $\\hatθ = \\frac1r \\bar{Y}$ is an unbiased estimator for θ.\n",
    "  2.  Show that $\\hatθ = \\frac1r \\bar{Y}$ is a minimum-variance estimator for θ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unbiasedness :\n",
    "\n",
    "    Compute the expectation of $Y$:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "        E(Y) &= \\frac{1}{(r-1)!\\theta^r}\\int_{0}^{\\infty} y^{r} e^{-y/\\theta}dy \\\\\n",
    "        &= \\frac{1}{(r-1)!\\theta^r}\\{[-\\theta y^{r} e^{-y/\\theta}]^{\\infty}_{0} - (-\\theta) r \\int_{0}^{\\infty} y^{r-1}e^{-y/\\theta}dy\\}   \\\\\n",
    "        &...(\\text{Repeat for r times})...  \\\\\n",
    "        &= \\frac{1}{(r-1)!\\theta^r}\\{ \\theta^r r! \\int_{0}^{\\infty} e^{-y/\\theta}dy\\}   \\\\\n",
    "        &= \\frac{1}{(r-1)!\\theta^r}\\{ \\theta^{r+1}r! [e^{-y/\\theta}]^{\\infty}_0\\} \\\\\n",
    "        &= \\frac{\\theta^{r+1}r!}{(r-1)!\\theta^r} \\\\\n",
    "        &= \\theta r\n",
    "\\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ \n",
    "    By the CLT, when $n$ is sufficiently large, $  \\bar{Y} \\sim N(\\mu,\\frac{\\sigma^2}{n})  $.\n",
    "\n",
    "    Thus, we have $$ E(\\hat{\\theta}) = E(\\frac1r \\bar{Y}) = \\theta $$\n",
    "\n",
    "    So, $\\hat{\\theta}$ is an unbiased estimator for $\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Efficiency:\n",
    "\n",
    "    Compute the log-pdf of $Y$: $$ l(\\theta) = -r\\ln(\\theta)-\\ln[(r-1)!]+ (r-1)\\ln(y)-\\frac{y}{\\theta} $$\n",
    "\n",
    "    Derive the score function: $$ \\frac{\\partial l(\\theta)}{\\partial \\theta} = -\\frac{r}{\\theta} + \\frac{y}{\\theta^2} $$\n",
    "\n",
    "    Compute the Fisher information: $$ I(\\theta) = -E(\\frac{\\partial^2 l(\\theta)}{\\partial \\theta^2}) = -E(\\frac{r}{\\theta^2} - 2\\frac{y}{\\theta^3}) = -\\frac{r}{\\theta^2} + 2\\frac{r}{\\theta^2} = \\frac{r}{\\theta^2} $$\n",
    "\n",
    "    Thus, we obtain the Cramer-Rao lower bound: $$ Var({\\hat{\\theta}}) \\geq B(\\theta) = \\frac{d[E(\\hat{\\theta})]}{d\\theta}(n\\times I(\\theta))^{-1} = \\frac{\\theta^2}{n\\times r} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ \n",
    "    Compute the $Var({\\hat{\\theta}})$:\n",
    "\n",
    "\\begin{aligned}\n",
    "        Var({\\bar{Y}}) &= \\frac{Var(Y)}{n}   \\\\\n",
    "        &= \\frac1n E(Y^2) - \\frac{\\theta^2 r^2}{n} \\\\\n",
    "        &= \\frac1n \\int_{0}^{\\infty} \\frac{1}{(r − 1)!θ^r}y^{r+1}e^{−y/θ} dy - \\frac{\\theta^2  r^2}{n}   \\\\\n",
    "        &= \\frac{1}{n(r − 1)!θ^r} \\int_{0}^{\\infty} y^{r+1}e^{−y/θ} dy - \\frac{\\theta^2 r^2}{n}  \\\\\n",
    "        &= \\frac{\\theta^{r+1}(r+1)!}{n(r − 1)!θ^r} \\int_{0}^{\\infty} e^{−y/θ} dy - \\frac{\\theta^2 r^2}{n}   \\\\\n",
    "        &= -\\frac{\\theta^2 r(r+1)}{n}   [e^{−y/θ}]^{\\infty}_{0} - \\frac{\\theta^2 r^2}{n} \\\\\n",
    "        &= \\frac{\\theta^2 r(r+1)}{n}- \\frac{\\theta^2 r^2}{n}    \\\\\n",
    "        &= \\frac{\\theta^2 r}{n}\n",
    "\\end{aligned}\n",
    "\n",
    "+ \n",
    "    $$ Var({\\hat{\\theta}}) = \\frac{Var({\\bar{Y}})}{r^2} = \\frac{\\theta^2}{n\\times r} = B(\\theta) $$\n",
    "\n",
    "    Thus, $\\hat{\\theta}$ is a minium-variance estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let $Y_1,Y_2,...,Y_n$ be a random sample of size n from the pdf of Question 5.5.6, $$f_Y(y;θ) =\\frac{1}{(r − 1)!θ^r}y^{r−1}e^{−y/θ}, 0 ≤ y $$ for positive parameter θ and r a known positive integer.\n",
    "+ Find a sufficient statistic for θ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    f_{Y^n}(y^n;\\theta) &= \\prod_{i=1}^{n} \\frac{1}{(r-1)!\\theta^r} y_i^{r-1} e^{-y_i/\\theta}   \\\\\n",
    "    &= [\\frac{1}{(r-1)!\\theta^r}]^n \\prod_{i=1}^{n} y_i^{r-1} e^{-y_i/\\theta}   \\\\\n",
    "    &= [\\frac{1}{(r-1)!\\theta^r}]^n e^{-(\\sum_{i=1}^n y_i)/n\\theta} \\prod_{i=1}^{n} y_i^{r-1}  \\\\\n",
    "    &= [\\frac{1}{(r-1)!\\theta^r}]^n e^{-(\\sum_{i=1}^n y_i)/n\\theta} e^{(r-1)^{-1}\\sum_{i=1}^n \\ln(y_i)} \\\\\n",
    "    &= \\frac{e^{-(\\sum_{i=1}^n y_i)/n\\theta}}{\\theta^r} \\cdot \\frac{e^{(r-1)^{-1}\\sum_{i=1}^n \\ln(y_i)}}{(r-1)!}    \\\\\n",
    "    &= g(T,\\theta)h(y^n)    \\space\\space(\\text{where }T = \\bar{Y} = \\frac{\\sum_{i=1}^n y_i}{n})\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $\\bar{Y} = \\frac{\\sum_{i=1}^n y_i}{n}$ is a sufficient statistic for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 5.6.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Suppose a random sample of size n is drawn from the pdf $$ f_Y(y;θ) = e^{−(y−θ)}, θ ≤ y $$\n",
    "1.  Show that $\\hatθ = Y_{\\min}$ is sufficient for the threshold parameter θ.\n",
    "2. Show that $Y_{\\max}$ is not sufficient for θ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Y_{\\min}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "        f_{Y^n}(y^n;\\theta) &= \\prod_{i=1}^{n} e^{-(y_i-\\theta)} \\cdot \\Bbb{1}_{y_i\\geq \\theta}   \\\\\n",
    "        &= e^{-(\\sum_{i=1}^{n} y_i) - n\\cdot \\theta}  \\prod_{i=1}^{n}\\Bbb{1}_{y_i\\geq \\theta}  \\\\\n",
    "        &= e^{-(\\sum_{i=1}^{n} y_i)}[e^{- n\\cdot \\theta}\\Bbb{1}_{y_{\\min}\\geq \\theta}] \\\\\n",
    "        &= h(y^n)g(\\theta,t = y_{\\min})\n",
    "\\end{aligned}\n",
    "\n",
    "+ \n",
    "    Thus, $T = Y_{\\min}$ is a sufficient statistic for $\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $Y_{\\max}$:\n",
    "\n",
    "    Let $T = Y_{\\max}$. Assume that $f_{Y^n}(y^n;\\theta) = h(y^n)g(\\theta,t = y_{\\max})$\n",
    "\n",
    "\\begin{aligned}\n",
    "        f_{Y^n}(y^n|t(y^n);\\theta) &= \\frac{f_{Y^n}(y^n;\\theta)}{P(T = t;\\theta)}  \\\\\n",
    "        &= \\frac{\\prod_{i=1}^{n} e^{-(y_i-\\theta)} \\cdot \\Bbb{1}_{y_i\\geq \\theta}}{g(y_{\\max},\\theta)}  \\\\\n",
    "        &= \\frac{\\prod_{i=1}^{n}e^{-y_i}\\cdot \\Bbb{1}_{y_i\\geq \\theta}}{g(y_{\\max},\\theta)e^{n\\theta}}  \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "+ \n",
    "    Since $\\prod_{i=1}^{n}e^{-y_i}\\cdot \\Bbb{1}_{y_i\\geq \\theta}$ can not be represented as $h(y^n)$,$T = Y_{\\max}$ is not a sufficient statistic for $\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第8题\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $\\{X_1,..., X_n\\}$ are i.i.d. samples. $S^2 = \\frac{1}{n(n-1)}\\sum_{i\\neq j}\\frac{(X_i - X_j)^2}{2}$ is called the U-statistic for $\\sigma^2$\n",
    "  + Prove that $S^2$ is unbiased for $\\sigma^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that $\\{X_1,..., X_n\\}$ are i.i.d., we have \n",
    "\n",
    "\\begin{aligned}\n",
    "    S^2 &= \\frac{1}{n(n-1)}\\sum_{i\\neq j} \\frac{(X_i-X_j)^2}{2} \\\\\n",
    "        &= \\frac{1}{n(n-1)} \\sum_{i\\neq j} \\frac{X_i^2 - 2X_i X_j + X_j^2}{2}   \\\\\n",
    "        &= \\frac{1}{n(n-1)} \\sum_{j = 1}^n \\sum_{1\\leq i < j} (X_i^2 - 2X_i X_j + X_j^2) \\\\\n",
    "        &= \\frac{1}{n(n-1)} \\sum_{j = 1}^n [X_j^2  -2X_j \\sum_{1\\leq i < j} X_i + \\sum_{1\\leq i < j} X_i^2] \\\\\n",
    "        &= \\frac{1}{n-1} \\sum_{j = 1}^n (X_j - \\bar{X})^2   \\\\\n",
    "        &= S_n^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $E(S^2) = E(S^2_n) = \\sigma^2$, implying that $S^2$ is unbiased for $\\sigma^2$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第9题"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Suppose $\\{X_1,..., X_n\\}$ are IID generated from Poisson(λ). Note that the mean and the variance of Poisson(λ) are both λ. Thus, we can use the sample mean $\\bar{X_n} =\\frac{1}{n}\\sum^{n}_{i=1} X_i$ and the sample variance $S_n^2 = (n − 1)^{−1}\\sum{i=1}^{n}(X_i − \\bar{X_n})^2$ to estimate λ.\n",
    "+ Furthermore, both estimators are unbiased for λ. Which is better? We need to use the simulation methods to get this answer.\n",
    "\n",
    "  + First, you randomly generate $\\{X_1,..., X_n\\}$ with $n = 20$ from Poisson(λ) where you can let $λ = 2$, and then you can compute two estimators. \n",
    "  + Second, you repeat the previous procedure 500 times, and then plot the respective histograms of the resulting 500 values of $\\bar{X_n}$ and $S_n^2$ and also calculate the variances of these 500 values of $\\bar{X_n}$ and $S_n^2$. \n",
    "  + Third, compare two histograms and two variances, draw your conclusion about which is better. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "from scipy.special import factorial\n",
    "from numpy import e\n",
    "import numpy as np\n",
    "ceil,lam = 10000,2\n",
    "x = np.arange(0,ceil)\n",
    "pmf = lam**x/factorial(x)*e**(-lam)\n",
    "pmf = np.where(pmf<0,0,pmf)\n",
    "poisson = rv_discrete(values=(x,pmf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bar = []\n",
    "S_square = []\n",
    "iter_times,n = 500,20\n",
    "for i in range(iter_times):\n",
    "    sample = poisson.rvs(size = n)\n",
    "    X_bar.append(np.mean(sample))\n",
    "    S_square.append(np.sum((sample - np.mean(sample))**2)/(n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "labels = [\"X_bar\",\"S_squared\"]\n",
    "bplot = ax.boxplot([X_bar,S_square],\n",
    "                     vert=True,  \n",
    "                     patch_artist=True, \n",
    "                     labels=labels)  \n",
    "ax.set_title('Estimates box plot')\n",
    "colors = ['pink', 'lightblue']\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor(color) #type: ignore\n",
    "ax.hlines(y = 2,xmin=-1,xmax=3,colors=['r'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "230b3ff13fbf17182562d98b6b92d0993293f138d9d821fa36058128ba3119c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
